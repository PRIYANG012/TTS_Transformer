
seed: 1234
__set_seed: !apply:torch.manual_seed [!ref <seed>]
output_folder: !ref ./result/TTS/<seed>
save_folder: !ref ./result/save
train_log: !ref ./result/train_log.txt
epochs: 175
device: "cuda:0"
keep_checkpoint_interval: 50

################Data Files#################
# Data files and pre-processing #
data_folder: ./extracted_data/LJSpeech-1.1 # e.g, /localscratch/ljspeech

train_json: !ref <save_folder>/train.json
valid_json: !ref <save_folder>/valid.json
test_json: !ref <save_folder>/test.json

splits: ["train", "valid","test"]
split_ratio: [70, 20, 10]

skip_prep: False

# Use the original preprocessing from nvidia
# The cleaners to be used (applicable to nvidia only)
text_cleaners: ['english_cleaners']
#################################


###############Audio Params#################
# Sounds transformations params
sr: 22050
n_fft: 2048
n_stft: 1025
frame_shift: 0.0125
hop_length: 256
frame_length: 0.05
win_length: 1024
mel_freq: 128
max_mel_time: 1024
max_db: 100
scale_db: 10
ref: 4.0
power: 2.0
norm_db: 10
ampl_multiplier: 10.0
ampl_amin: 1e-10
db_multiplier: 1.0
ampl_ref: 1.0
ampl_power: 1.0
################################


#################Progress##################
# Progress Samples                #
# Progress samples are used to monitor the progress
# of an ongoing training session by outputting samples
# of spectrograms, alignments, etc at regular intervals
# Whether to enable progress samples
progress_samples: True

# The path where the samples will be stored
progress_sample_path: !ref ./result/samples
# The interval, in epochs. For instance, if it is set to 5,
# progress samples will be output every 5 epochs
progress_samples_interval: 1
# The sample size for raw batch samples saved in batch.pth
# (useful mostly for model debugging)
progress_batch_sample_size: 3

progress_sample_logger: !new:speechbrain.utils.train_logger.ProgressSampleLogger
  output_path: !ref <progress_sample_path>
  batch_sample_size: !ref <progress_batch_sample_size>
  formats:
    raw_batch: raw



###############Hyperparameters#################
# Optimization Hyperparameters #
lr_start: 0.001
lr_final: 0.0001
weight_decay: 0.000006
batch_size: 32 #minimum 2
num_workers: 0
text_num_embeddings : 86
embedding_size : 256
encoder_embedding_size : 512
dim_feedforward : 1024
postnet_embedding_size : 1024
encoder_kernel_size : 3
postnet_kernel_size : 5
grad_clip : 1.0
r_gate : 1.0

###############Data Processing################

audio_feature_extractor: !new:model.AudioFeatureExtractor
  sr: !ref <sr>
  n_fft: !ref <n_fft>
  n_stft: !ref <n_stft>
  frame_shift: !ref <frame_shift>
  hop_length: !ref <hop_length>
  frame_length: !ref <frame_length>
  win_length: !ref <win_length>
  mel_freq: !ref <mel_freq>
  max_mel_time: !ref <max_mel_time>
  max_db: !ref <max_db>
  scale_db: !ref <scale_db>
  ref: !ref <ref>
  power: !ref <power>
  norm_db: !ref <norm_db>
  ampl_multiplier: !ref <ampl_multiplier>
  ampl_amin: !ref <ampl_amin>
  db_multiplier: !ref <db_multiplier>
  ampl_ref: !ref <ampl_ref>
  ampl_power: !ref <ampl_power>


train_dataloader_opts:
  batch_size: !ref <batch_size>
  drop_last: False  #True #False
  num_workers: !ref <num_workers>
  collate_fn: !name:model.TTSCollate

valid_dataloader_opts:
  batch_size: !ref <batch_size>
  num_workers: !ref <num_workers>
  collate_fn: !name:model.TTSCollate

test_dataloader_opts:
  batch_size: !ref <batch_size>
  num_workers: !ref <num_workers>
  collate_fn: !name:model.TTSCollate


########### model ######################
model: !new:model.TransformerTTS
  device : !ref <device>
  text_num_embeddings: !ref <text_num_embeddings>
  embedding_size: !ref <embedding_size>
  encoder_embedding_size: !ref <encoder_embedding_size>
  dim_feedforward: !ref <dim_feedforward>
  postnet_embedding_size: !ref <postnet_embedding_size>
  encoder_kernel_size: !ref <encoder_kernel_size>
  postnet_kernel_size: !ref <postnet_kernel_size>
  max_mel_time: !ref <max_mel_time>
  mel_freq: !ref <mel_freq>

modules:
  model: !ref <model>

criterion: !new:model.TTSLoss

############optimizer#####################
opt_class: !name:torch.optim.Adam
  lr: !ref <lr_start>
  weight_decay: !ref <weight_decay>

###########epoch object###################
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
  limit: !ref <epochs>

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: !ref <train_log>

############annealing_function#############
lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler
    initial_value: !ref <lr_start>
    final_value: !ref <lr_final>
    epoch_count: !ref <epochs>

##################checkpointer###############
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: !ref <save_folder>
  recoverables:
    model: !ref <model>
    counter: !ref <epoch_counter>


pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
    loadables:
        model: !ref <model>

# Text to sequence module
text_to_sequence: !new:model.AudioFeatureExtractor

